# -*- coding: utf-8 -*-
"""Local_News

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GvRswsCA4boS7bNbNhFxECq_ysFF9pk7
"""

# 라이브러리 import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
import time
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
from dateutil import parser
import re
from newspaper import Article
import requests
import numpy as np
from google.colab import drive
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

def initialize_chrome_driver():
  # Chrome 옵션 설정 : USER_AGENT는 알아서 수정
  #USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.6045.105 Safari/537.36"
  # 태준컴
  USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5.2 Safari/605.1.15'
  chrome_options = Options()
  chrome_options.page_load_strategy = 'normal'  # 'none', 'eager', 'normal'
  chrome_options.add_argument('--headless')
  chrome_options.add_argument('--no-sandbox')
  chrome_options.add_argument('--disable-dev-shm-usage')
  chrome_options.add_argument('--disable-gpu')
  chrome_options.add_argument(f'user-agent={USER_AGENT}')
  # Chrome 드라이버 설정
  service = Service()
  wd = webdriver.Chrome(service=service, options=chrome_options)
  return wd

# 날짜 통합 함수
def date_util(article_date):
  today = datetime.now().date()
  try:
    # Parse the date using dateutil.parser
    article_date = parser.parse(article_date).date()
  except ValueError:
    # If parsing fails, handle the relative dates
    article_date = article_date.lower()
    time_keywords = ["AM", "PM", "h", "hrs", "hr", "m", "s", "hours","hour", "minutes", "minute", "mins", "min", "seconds", "second", "secs", "sec"]
    if any(keyword in article_date for keyword in time_keywords):
      article_date = today
    elif "days" in article_date or "day" in article_date:
      # Find the number of days and subtract from today
      number_of_days = int(''.join(filter(str.isdigit, article_date)))
      article_date = today - timedelta(days=number_of_days)
    else:
      return None
  return article_date

# 에러 메시지 작성 함수
def Error_Message(message, add_error):
    if message is not str() : message += '/'
    message += add_error
    return message

# 데이터프레임
articles = []
error_list = []
today_list = [date_util((datetime.now()- timedelta(days=1)).strftime("%Y-%m-%d")), date_util((datetime.now()).strftime("%Y-%m-%d"))]
today = date_util(datetime.now().strftime("%Y-%m-%d"))

# urls
# Georgia
url_local_1 = 'https://www.ajc.com/news/'
url_local_2 = 'valdostadailytimes.com'

# California
url_local_3 = 'https://www.mercurynews.com/latest-headlines/'
url_local_4 = 'https://www.bizjournals.com/news/'

# Texas
url_local_5 = 'https://www.dallasnews.com/news/'
url_local_6 = 'https://www.expressnews.com/news/'

# New York
url_local_7 = 'https://buffalonews.com/news/#tracking-source=main-nav'
url_local_8 = 'syracuse.com'

# New Jersey
url_local_9 = 'https://www.northjersey.com/news/'
url_local_10 = 'https://www.nj.com/#section__news'

# North Carolina
url_local_11 = 'charlotteobserver.com'
url_local_12 = 'newsobserver.com'

# District of Columbia
url_local_13 = 'https://www.washingtonpost.com/latest-headlines/'
url_local_14 = 'https://www.washingtontimes.com/news/world/'

# Virginia
url_local_15 = 'roanoke.com'
url_local_16 = 'richmond.com'

# Maryland
url_local_17 = 'https://www.baltimoresun.com/latest-headlines/'
url_local_18 = 'https://www.savannahnow.com/news/local/'

###############################################<직접 크롤링 : url_local_1, 3, 5, 9, 13, 14, 17>###############################################
# 직접 scraping시, 본문이 긁혀지지 않을 때(Article이 사용되지 않을때), context 긁는 함수
def scrap_context1(url):
    wd = initialize_chrome_driver()
    wd.get(url)
    time.sleep(3)
    html = wd.page_source
    soup = BeautifulSoup(html, 'html.parser')

    # 키워드가 포함된 모든 텍스트 블록 찾기
    context = str()
    # 대소문자 구분 없이 키워드와 정확히 일치하는 정규 표현식 패턴 생성
    pattern = re.compile('|'.join(r'(?<!\w)' + re.escape(keyword) + r'(?!\w)' for keyword in Top50_Name_list), re.IGNORECASE)
    #for element in soup.find_all(string=lambda text: keyword in text):
    for element in soup.find_all(string=pattern):
        context += (element.text + '\n')

    # 기업명 앞+뒤 문맥 파악할 수 있는 글이 포함되면, context return
    if context :
      return(context)
    else:
      return()
###############################################<url_local_1>###############################################
#url_local_1 = "https://www.ajc.com/news/"
wd = initialize_chrome_driver()
wd.get(url_local_1)
time.sleep(5)
html = wd.page_source
soup = BeautifulSoup(html, 'html.parser')
error_message = str()
local = '[Georgia] '+url_local_1
try:
  news_blocks = soup.find_all('div',class_=['c-homeList', 'home-headline', 'headline', 'c-homeList left-photo','c-homeList no-photo','headline-box'])
  if not news_blocks: error_list.append({'Error Link': url_local_1, 'Error': "None News"})
  else:
    for block in news_blocks:
        date = block.find('span', class_='isTease article-timestamp')
        if not date: continue
        article_date = date_util(date.text.strip())
        if not article_date: error_message = Error_Message(error_message, "None Date")
        if article_date in today_list:
          link = block.find('a')['href']
          article_link = f'https://www.ajc.com{link}'
          if not article_link: error_message = Error_Message(error_message, "None Link")
          article = Article(article_link, language = 'en') # URL과 언어를 입력
          article.download()
          article.parse()
          title = article.title
          if not title: error_message = Error_Message(error_message, "None Title")
          content = article.text
          if not content: content = scrap_context1(article_link)
          if error_message is not str():
                error_list.append({
                  'Error Link': url_local_1,
                  'Error': error_message
                })
          else:
              if content:
                articles.append({
                  'Local Site': local,
                  'Title': title,
                  'Link': article_link,
                  'Contents': content
                })
except Exception as e:
  error_list.append({
      'Error Link': url_local_1,
      'Error': str(e)
      })
###############################################<url_local_3>###############################################
# url_local_3 = "https://www.mercurynews.com/latest-headlines/"
wd = initialize_chrome_driver()
wd.get(url_local_3)
time.sleep(5)
html = wd.page_source
soup = BeautifulSoup(html, 'html.parser')
error_message = str()
local = '[California] '+url_local_3
news_items = soup.find_all('a', class_='article-title')
for item in news_items:
    link = item['href']
    if not link: error_message = Error_Message(error_message, "None Link")
    article = Article(link, language='en')
    article.download()
    article.parse()
    article_date = date_util(str(article.publish_date))
    if not article_date : error_message = Error_Message(error_message, "None Date")
    else:
      if article_date in today_list :
          title = article.title
          if not title : error_message = Error_Message(error_message, "None Title")
          text = article.text
          if not text: text = scrap_context1(link)
          if error_message != str():
                    error_list.append({
                        'Error Link': url_local_3,
                        'Error': error_message
                    })               
          else:
              if text:
                articles.append({
                    'Local Site': local,
                    'Title': title,
                    'Link': link,
                    'Content(RAW)': text
                })
###############################################<url_local_5>###############################################
#url_local_5 = "https://www.dallasnews.com/news/"
wd = initialize_chrome_driver()
wd.get(url_local_5)
time.sleep(5)
for _ in range(5):
  button = wd.find_element(By.ID, 'load-more')
  button.click()
  time.sleep(1)
html = wd.page_source
soup = BeautifulSoup(html, 'html.parser')
error_message = str()
local = '[Texas] '+url_local_5
try:
  news_blocks = soup.find_all('div', class_='dmnc_generic-cards-card-module__FcjA3 w-full py-6 flex border-gray-light relative')
  if not news_blocks: error_list.append({'Error Link': url_local_5, 'Error': "None News"})
  for item in news_blocks :
    date_tag = item.find('span', class_='dmnc_generic-article-elements-article-elements-module__l-ds8 secondaryRoman secondaryRoman-10 capitalize text-gray-medium')
    date_string = date_tag.text.strip()
    article_date = date_util(date_string)
    if not article_date : error_message = Error_Message(error_message, "None Date")
    if article_date in today_list:
      link = item.find('a')['href']
      link = f'https://www.dallasnews.com{link}'
      if not link: error_message = Error_Message(error_message, "None link")
      article = Article(link, language = 'en') # URL과 언어를 입력
      article.download()
      article.parse()
      title = article.title
      if not title: error_message = Error_Message(error_message, "None Title")
      content = article.text
      if not content: content = scrap_context1(link)
      if error_message is not str():
                error_list.append({
                    'Error Link': url_local_5,
                    'Error': error_message
                    })
      else:
          if content:
                articles.append({
                  'Local Site': local,
                  'Title': title,
                  'Link': link,
                  'Content(RAW)': content
                  })
except Exception as e:
  error_list.append({
      'Error Link': url_local_5,
      'Error': str(e)
  })
###############################################<url_local_9>###############################################
# url_local_9 = 'https://www.northjersey.com/news/'
wd = initialize_chrome_driver()
wd.get(url_local_9)
time.sleep(5)
html = wd.page_source
soup = BeautifulSoup(html, 'html.parser')
error_message = str()
local = '[New Jersey] '+url_local_9
base_url = 'https://www.northjersey.com'
# 'gnt_x__nft gnt_m_flm_a' 클래스를 제외한 'gnt_m_flm_a' 클래스를 가진 모든 <a> 태그 찾기
news_items = soup.find_all('a', class_='gnt_m_flm_a')
for item in news_items:
    if 'gnt_x__nft' not in item['class']:
        link = base_url + item['href']
        if not link : error_message = Error_Message(error_message, "None Link")
        article = Article(link, language='en')
        article.download()
        article.parse()
        article_date = date_util(str(article.publish_date))
        if not article_date : error_message = Error_Message(error_message, "None Date")
        else:
            if article_date in today_list :
                title = article.title
                if not title : error_message = Error_Message(error_message, "None Title")
                text = article.text
                if not text: text = scrap_context1(link)
                if error_message != str():
                    error_list.append({
                        'Error Link': url_local_9,
                        'Error': error_message
                    })
                else:
                    if text:
                        articles.append({
                            'Local Site': local,
                            'Title': title,
                            'Link': link,
                            'Content(RAW)': text
                        })
###############################################<url_local_13>###############################################
# url_local_13 = 'https://www.washingtonpost.com/latest-headlines/'
wd = initialize_chrome_driver()
wd.get(url_local_13)
time.sleep(5)
html = wd.page_source
soup = BeautifulSoup(html, 'html.parser')
error_message = str()
local = '[District of Columbia] '+url_local_13
date_items = soup.find_all('span', class_='timestamp gray-dark')
for item in date_items:
    date = date_util(item.text.strip())
    if date in today_list:
        link = item.find_parent().find_parent().find('a')['href']
        if not link: error_message = Error_Message(error_message, "None Link")
        article = Article(link, language='en')
        article.download()
        article.parse()
        title = article.title
        if not title: error_message = Error_Message(error_message, "None Title")
        text = article.text
        if not text: text = scrap_context1(link)
        if error_message != str():
            error_list.append({
                'Error Link': url_local_13,
                'Error': error_message
            })
        else:
            if text:
                articles.append({
                    'Local Site': local,
                    'Title': title,
                    'Link': link,
                    'Content(RAW)': text
                })
###############################################<url_local_14>###############################################
# url_local_14 = "https://www.washingtontimes.com/news/world/"
wd = initialize_chrome_driver()
wd.get(url_local_14)
time.sleep(5)
html = wd.page_source
soup = BeautifulSoup(html, 'html.parser')
error_message = str()
local = '[District of Columbia] '+url_local_14
base_url = "https://www.washingtontimes.com"
news_items = soup.find_all('h2', class_='article-headline')
for item in news_items:
    relative_link = item.find('a')['href']
    link = base_url + relative_link
    article = Article(link, language='en')
    article.download()
    article.parse()
    article_date = date_util(str(article.publish_date))
    if not article_date : error_message = Error_Message(error_message, "None Date")
    else:
      if article_date in today_list :
          title = article.title
          if not title : error_message = Error_Message(error_message, "None Title")
          text = article.text
          if not text: text = scrap_context1(link)
      if error_message != str():
                error_list.append({
                    'Error Link': url_local_14,
                    'Error': error_message
                })
      else:
            if text:
                articles.append({
                    'Local Site': local,
                    'Title': title,
                    'Link': link,
                    'Content(RAW)': text
                })
###############################################<url_local_17>###############################################
# url_local_17 = 'https://www.baltimoresun.com/latest-headlines/'
wd = initialize_chrome_driver()
wd.get(url_local_17)
time.sleep(5)
html = wd.page_source
soup = BeautifulSoup(html, 'html.parser')
error_message = str()
local = '[Maryland] '+url_local_17
try:
    # 뉴스 아이템 추출
    news_items = soup.find_all('div', class_='article-info')
    if not news_items:
        error_list.append({
            'Error Link': url_local_17,
            'Error': "None News"
        })
    else:
        for item in news_items:
            link = item.find('a', class_='article-title')['href']
            if not link : error_message = Error_Message(error_message, "None Link")
            article = Article(link, language='en')
            article.download()
            article.parse()
            article_date = date_util(str(article.publish_date))
            if not article_date: error_message = Error_Message(error_message, "None Date")
            else:
              if article_date in today_list :
                  title = article.title
                  if not title : error_message = Error_Message(error_message, "None Title")
                  text = article.text
                  if not text: text = scrap_context1(link)
                if error_message is not str():
                  error_list.append({
                      'Error Link': url_local_17,
                      'Error': error_message
                      })
                else:
                    if text:
                        articles.append({
                            'Local Site': local,
                            'Title': title,
                            'Link': link,
                            'Content(RAW)': text
                        })
except Exception as e:
    error_list.append({
        'Error Link': url_local_17,
        'Error': str(e)
    })

###############################################<안 되는 사이트 : 빙 search url_local_2, 4, 6, 7, 8, 10, 11, 12, 15, 16, 18>###############################################

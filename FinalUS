# 라이브러리 import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
import time
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
from dateutil import parser
import re

# Chrome 옵션 설정 : USER_AGENT는 알아서 수정
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.6045.105 Safari/537.36"
chrome_options = Options()
chrome_options.page_load_strategy = 'normal'  # 'none', 'eager', 'normal'
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument(f'user-agent={USER_AGENT}')

# Chrome 드라이버 설정
service = Service()
wd = webdriver.Chrome(service=service, options=chrome_options)

# 날짜 통합 함수
def date_util(article_date):
  try:
    # Parse the date using dateutil.parser
    article_date = parser.parse(article_date).date()
  except ValueError:
    # If parsing fails, handle the relative dates
    article_date = article_date.lower()
    time_keywords = ["hours","hour", "minutes", "minute", "mins", "min", "seconds", "second", "secs", "sec"]
    if any(keyword in article_date for keyword in time_keywords):
      article_date = today
    elif "days" in article_date or "day" in article_date:
      # Find the number of days and subtract from today
      number_of_days = int(''.join(filter(str.isdigit, article_date)))
      article_date = today - timedelta(days=number_of_days)
    else:
      return None
  return article_date

# 에러 메시지 작성 함수
def Error_Message(message, add_error):
    if message is not str() : message += '/'
    message += add_error
    return message

# 데이터프레임
articles = []
error_list = []
today = datetime.now().date()

# 연방정부 + 방산업체들 + NASA
url_1 = 'https://www.state.gov/press-releases/'
url_2 = 'https://www.state.gov/department-press-briefings/'
url_3 = 'https://home.treasury.gov/news/press-releases'
url_4 = 'https://www.defense.gov'
url_5 = 'https://www.army.mil/news'
url_6 = 'https://www.navy.mil/Press-Office/'
url_7 = 'https://www.af.mil/News/Category/22750/'
url_8 = 'https://www.nsa.gov/Press-Room/Press-Releases-Statements/'
url_9 = 'https://www.justice.gov/news'
url_10 = 'https://www.fbi.gov/news/press-releases'
url_11 = 'https://www.doi.gov/news'
url_12 = 'https://www.usda.gov/media/press-releases'
url_13 = 'https://www.ars.usda.gov/news-events/news-archive/'
url_14 = 'https://www.fs.usda.gov/news/releases'
url_15 = 'https://www.fas.usda.gov/newsroom/search'
url_16 = 'https://www.commerce.gov/news'
url_17 = 'https://www.dol.gov/newsroom/releases?agency=All&state=All&topic=All&year=all&page=0'
url_18 = 'https://www.hhs.gov/about/news/index.html'
url_19 = 'https://www.fda.gov/news-events/fda-newsroom/press-announcements'
url_20 = 'https://www.fda.gov/news-events/speeches-fda-officials'
url_21 = 'https://www.transportation.gov/newsroom/press-releases'
url_22 = 'https://www.transportation.gov/newsroom/speeches'
url_23 = 'https://www.energy.gov/newsroom'
url_24 = 'https://www.ed.gov/news/press-releases'
url_25 = 'https://www.ed.gov/news/speeches'
url_26 = 'https://news.va.gov/news/'
url_27 = 'https://www.dhs.gov/news-releases/press-releases'
url_28 = 'https://www.dhs.gov/news-releases/speeches'
url_29 = 'https://www.fema.gov/about/news-multimedia/press-releases'
url_30 = 'https://www.secretservice.gov/newsroom'
url_31 = 'https://www.epa.gov/newsreleases/search'
url_32 = 'https://www.lockheedmartin.com/en-us/news.html'
url_33 = 'https://boeing.mediaroom.com/news-releases-statements'
url_34 = 'https://www.rtx.com/news'
url_35 = 'https://news.northropgrumman.com/news/releases'
url_36 = 'https://www.gd.com/news/news-feed?page=0&types=Press Release'
url_37 = 'https://www.baesystems.com/en/newsroom'
url_38 = 'https://www.l3harris.com/ko-kr/newsroom/search?size=n_10_n&sort-field%5Bname%5D=Publish%20Date&sort-field%5Bvalue%5D=created_date&sort-field%5Bdirection%5D=desc&sort-direction='
url_39 = 'https://investor.textron.com/news/news-releases/default.aspx'
url_40 = 'https://www.nasa.gov/news/all-news/'

# Georgia
url_41 = 'https://sos.ga.gov/news/division/31?page=0'
url_42 = 'https://gov.georgia.gov/press-releases'
url_43 = 'https://dol.georgia.gov/latest-news'
url_44 = 'https://www.georgia.org/press-releases'
url_45 = 'https://www.gachamber.com/all-news/'

# California
url_46 = 'https://www.sos.ca.gov/administration/news-releases-and-advisories/2023-news-releases-and-advisories'
url_47 = 'https://www.gov.ca.gov/newsroom/'
url_48 = 'https://business.ca.gov/newsroom/'
url_49 = 'https://business.ca.gov/calosba-latest-news/'
url_50 = 'https://www.dir.ca.gov/dlse/DLSE_whatsnew.htm'
url_51 = 'https://www.dir.ca.gov/dosh/DOSH_Archive.html'
url_52 = 'https://www.dir.ca.gov/mediaroom.html'
url_53 = 'https://news.caloes.ca.gov/'
url_54 = 'https://advocacy.calchamber.com/california-works/calchamber-members-in-the-news/'

# Texas
url_55 = 'https://gov.texas.gov/'
url_56 = 'https://www.texasattorneygeneral.gov/news'
url_57 = 'https://www.txdot.gov/about/newsroom/statewide.html'
url_58 = 'https://www.dps.texas.gov/'
url_59 = 'https://www.twc.texas.gov/'
url_60 = 'https://tpwd.texas.gov/'
url_61 = 'https://comptroller.texas.gov/'
url_62 = 'https://www.tdi.texas.gov/index.html'
url_63 = 'https://www.txbiz.org/chamber-news'
url_64 = 'https://www.txbiz.org/press-releases'

# New York
url_65 = 'https://www.governor.ny.gov/news'
url_66 = 'https://www.ny.gov/'
url_67 = 'https://www.dot.ny.gov/news/press-releases/2023'
url_68 = 'https://ag.ny.gov/press-releases'
url_69 = 'https://www.dfs.ny.gov/'
url_70 = 'https://www.tax.ny.gov/'
url_71 = 'https://chamber.nyc/news'
url_72 = 'https://aging.ny.gov/'
url_73 = 'https://www.nyserda.ny.gov/'

# New Jersey
url_74 = 'https://www.njchamber.com/press-releases'
url_75 = 'https://www.nj.gov/health/news/'
url_76 = 'https://www.nj.gov/mvc/news/news.htm'

# North Carolina
url_77 = 'https://sosnc.gov/news_events/press_releases'
url_78 = 'https://www.commerce.nc.gov/news/press-releases'
url_79 = 'https://www.commerce.nc.gov/news/feed'
url_80 = 'https://www.ncdor.gov/news/press-releases'
url_81 = 'https://www.iprcenter.gov/news'
url_82 = 'https://edpnc.com/news-events/'
url_83 = 'https://ncchamber.com/category/chamber-updates/'

# 워싱턴 dc
url_84 = 'https://dc.gov/newsroom'
url_85 = 'https://dcchamber.org/posts/'
url_86 = 'https://planning.dc.gov/newsroom'
url_87 = 'https://dpw.dc.gov/newsroom'

# Virginia
url_88 = 'https://www.governor.virginia.gov/newsroom/news-releases/'
url_89 = 'https://www.vedp.org/press-releases'
url_90 = 'https://www.doli.virginia.gov/category/announcements/'
url_91 = 'https://vachamber.com/news/'

# Maryland
url_92 = 'https://governor.maryland.gov/news/press/Pages/default.aspx?page=1'
url_93 = 'https://news.maryland.gov/mde/category/press-release/'
url_94 = 'https://commerce.maryland.gov/media/press-room'
url_95 = 'https://www.dllr.state.md.us/whatsnews/'
url_96 = 'https://www.mdchamber.org/news/'

urls = [url_1, url_2, url_3, url_4, url_5, url_6, url_7, url_8, url_9, url_10, url_11, url_12, url_13, url_14, url_15, url_16, url_17, url_18, url_19, url_20,
        url_21, url_22, url_23, url_24, url_25, url_26, url_27, url_28, url_29, url_30, url_31, url_32, url_33, url_34, url_35, url_36, url_37, url_38, url_39, url_40,
        url_41, url_42, url_44, url_45, url_46, url_47, url_48, url_49, url_50, url_51, url_52, url_53, url_54, url_55, url_56, url_57, url_58, url_59, url_60,
        url_61, url_62, url_63, url_64, url_65, url_67, url_68, url_71, url_74, url_75, url_76, url_78, url_80,
        url_81, url_82, url_83, url_84, url_86, url_87, url_88, url_89, url_90, url_91, url_92, url_93, url_94, url_95, url_96]

########################################### <4> ##############################################
#url_4 = 'https://www.defense.gov'
wd = webdriver.Chrome(service=service, options=chrome_options)
wd.get(url_4)
time.sleep(5)
html = wd.page_source
soup = BeautifulSoup(html, 'html.parser')
error_message = str()
date_blocks, article_date = None, None
try:
  date_blocks = soup.find_all('time')
  if not date_blocks: error_list.append({'Error Link': url_4, 'Error': "Date Blocks"})
  else:
    for block in date_blocks:
      date_str = block['data-dateap']
      article_date = date_util(date_str)
      article_link, title, bodys = None, None, None
      if article_date == today:
        article_link = block.find_parent().find_parent().find_parent().find_parent().find('a').find_next('a')['href']
        if not article_link: error_message = Error_Message(error_message, "None Link")
        wd = webdriver.Chrome(service=service, options=chrome_options)
        wd.get(article_link)
        time.sleep(5)
        article_html = wd.page_source
        article_soup = BeautifulSoup(article_html, 'html.parser')
        title = article_soup.find('h1', class_='maintitle').text.strip()
        if not title: error_message = Error_Message(error_message, "None Title")
        # 기사 본문을 찾습니다.
        body = [] ; bodys = str()
        # 조건에 맞는 요소를 찾는 함수
        def custom_filter(tag):
          # 'p' 태그는 클래스에 상관없이 모두 선택
          if tag.name == 'p':
            return True
          # 'div' 태그는 'ast-glance' 클래스만 선택
          if tag.name == 'div' and 'ast-glance' in tag.get('class', []):
            return True
          # 그 외의 경우는 선택하지 않음
          return False
        paragraphs = article_soup.find_all(custom_filter)
        for p in paragraphs: body.append(p.get_text().strip())
        for i in range(3,len(body)-2): bodys += str(body[i]).strip()
        if not bodys: error_message = Error_Message(error_message, "None Contents")
        if error_message is not str():
          error_list.append({
            'Error Link': url_4,
            'Error': error_message
          })
        else:
          articles.append({
            'Title': title,
            'Link': article_link,
            'Content(Raw)': bodys
          })
except Exception as e:
  error_list.append({
      'Error Link': url_4,
      'Error': str(e)
      })
########################################### <6> ##############################################
#url_6 = 'https://www.navy.mil/Press-Office/'
wd = webdriver.Chrome(service=service, options=chrome_options)
wd.get(url_6)
time.sleep(5)
html = wd.page_source
soup = BeautifulSoup(html, 'html.parser')
error_message = str()
news_blocks, article_date, article_link, title, bodys = None, None, None, None, None
try:
  news_blocks = soup.find_all('div', class_='col-12 col-sm-8')
  if not news_blocks: error_list.append({'Error Link': url_6, 'Error': "None News"})
  else:
    for article in news_blocks:
      date_str = article.find('h6').text
      article_date = date_util(date_str)
      article_link, title, bodys = None, None, None
      if article_date == today:
        article_link = article.find('h2').find('a')['href']
        if not article_link: error_message = Error_Message(error_message, "None Link")
        wd = webdriver.Chrome(service=service, options=chrome_options)
        wd.get(article_link)
        time.sleep(5)
        article_html = wd.page_source
        article_soup = BeautifulSoup(article_html, 'html.parser')
        title = article.find('h2').get_text().strip()
        if not title: error_message = Error_Message(error_message, "None Title")
        # 기사 본문을 찾습니다.
        body = [] ; bodys = str()
        paragraphs = article_soup.find('div', class_='acontent-container').find_all('p')
        for p in paragraphs: body.append(p.get_text().strip())
        for i in range(len(body)): bodys += str(body[i]).strip()
        if not bodys: error_message = Error_Message(error_message, "None Contents")
        if error_message is not str():
          error_list.append({
            'Error Link': url_6,
            'Error': error_message
          })
        else:
          articles.append({
            'Title': title,
            'Link': article_link,
            'Content(Raw)': bodys
          })
except Exception as e:
  error_list.append({
      'Error Link': url_6,
      'Error': str(e)
      })
########################################### <10> ##############################################
#url_10 = 'https://www.fbi.gov/news/press-releases'
wd = webdriver.Chrome(service=service, options=chrome_options)
wd.get(url_10)
time.sleep(5)
html = wd.page_source
soup = BeautifulSoup(html, 'html.parser')
error_message = str()
date_blocks, article_date, article_link, title, bodys = None, None, None, None, None
try:
  date_blocks = soup.find_all('p',class_='date')
  if not date_blocks: error_list.append({'Error Link': url_10, 'Error': "Date Blocks"})
  else:
    for block in date_blocks:
      date_str = block.text.strip()
      article_date = date_util(date_str)
      article_link, title, bodys = None, None, None
      if article_date == today:
        article_link = block.find_parent().find_parent().find('a')['href']
        if not article_link: error_message = Error_Message(error_message, "None Link")
        wd = webdriver.Chrome(service=service, options=chrome_options)
        wd.get(article_link)
        time.sleep(5)
        article_html = wd.page_source
        article_soup = BeautifulSoup(article_html, 'html.parser')
        title = block.find_parent().find_parent().find('a').text.strip()
        if not title: error_message = Error_Message(error_message, "None Title")
        bodys = article_soup.find('div',class_=['node-body','mosaic-tile-content']).text.strip()
        if not bodys: error_message = Error_Message(error_message, "None Contents")
        if error_message is not str():
          error_list.append({
            'Error Link': url_10,
            'Error': error_message
          })
        else:
          articles.append({
            'Title': title,
            'Link': article_link,
            'Content(Raw)': bodys
          })
except Exception as e:
  error_list.append({
      'Error Link': url_10,
      'Error': str(e)
      })
########################################### <14> ##############################################
 # url_14 = 'https://www.fs.usda.gov/news/releases'
 wd = webdriver.Chrome(service=service, options=chrome_options)
 wd.get(url_14)
 time.sleep(5)
 html = wd.page_source
 soup = BeautifulSoup(html, 'html.parser')
 error_message = str()

 try:
     news_items = soup.find_all('div', class_='margin-bottom-2 views-row')
     if not news_items:
         error_list.append({
             'Error Link': url_14,
             'Error': "None News"
         })
     else:
         for item in news_items:
             date_tag = item.find('time', class_='text-base-darker')
             date_str = date_tag.get_text(strip=True)
             article_date = date_util(date_str)
             if not article_date:
                 error_message = Error_Message(error_message, "None Date")
             if article_date == today:
                 title_tag = item.find('span', class_='field-content featured-title')
                 title = title_tag.get_text(strip=True)
                 if not title:
                     error_message = Error_Message(error_message, "None Title")
                 article_link = f"https://www.fs.usda.gov{item.a['href']}"
                 if not article_link:
                     error_message = Error_Message(error_message, "None Link")
                 wd.get(article_link)
                 article_html = wd.page_source
                 article_soup = BeautifulSoup(article_html, 'html.parser')
                 content_div = article_soup.find('div', class_='usa-prose full-width')
                 article_body = ' '.join(p.get_text() for p in content_div.find_all('p')) #if content_div else "No content available."
                 if not article_body:
                     error_message = Error_Message(error_message, "None Contents")
                 if error_message != "":
                     error_list.append({
                         'Error Link': url_14,
                         'Error': error_message
                     })
                 else:
                     articles.append({
                         'Title': title,
                         'Link': article_link,
                         'Content(Raw)': article_body
                     })
 except Exception as e:
     error_list.append({
         'Error Link': url_14,
         'Error': str(e)
     })
 ########################################### <41> ##############################################
 #url_41 = 'https://sos.ga.gov/news/division/31?page=0'
 wd = webdriver.Chrome(service=service, options=chrome_options)
 time.sleep(5)
 html = wd.page_source
 soup = BeautifulSoup(html, 'html.parser')
 error_message = str()

 try:
     news_items = soup.select(".card__content")
     if not news_items:
         error_list.append({
             'Error Link': url_41,
             'Error': "None News"
         })
     else:
         for item in news_items:
             date_str = item.select_one('.card__date').get_text(strip=True)
             article_date = date_util(date_str)
             if not article_date:
                 error_message = Error_Message(error_message, "None Date")

             if article_date == today:
                 title = item.select_one('.heading__link').get_text(strip=True)
                 if not title:
                     error_message = Error_Message(error_message, "None Title")

                 article_link = f"https://sos.ga.gov{item.a['href']}"
                 if not article_link:
                     error_message = Error_Message(error_message, "None Link")

                 wd.get(article_link)
                 article_html = wd.page_source
                 article_soup = BeautifulSoup(article_html, 'html.parser')

                 paragraphs = article_soup.select("div.layout-2x__content p")
                 article_body = ' '.join(p.get_text(strip=True) for p in paragraphs)
                 if not article_body:
                     error_message = Error_Message(error_message, "None Contents")

                 if error_message != "":
                     error_list.append({
                         'Error Link': url_41,
                         'Error': error_message
                     })
                 else:
                     articles.append({
                         'Title': title,
                         'Link': article_link,
                         'Content(Raw)': article_body
                     })

 except Exception as e:
     error_list.append({
         'Error Link': url_41,
         'Error': str(e)
     })




 ########################################### <42> ##############################################

 #url_42 = 'https://gov.georgia.gov/press-releases'
 wd = webdriver.Chrome(service=service, options=chrome_options)
 time.sleep(5)
 html = wd.page_source
 soup = BeautifulSoup(html, 'html.parser')
 error_message = str()
 # today = datetime(2023, 11, 8).date()     # For Test

 try:
     news_items = soup.select(".news-teaser")
     if not news_items:
         error_list.append({
             'Error Link': url_42,
             'Error': "None News"
         })
     else:
         for item in news_items:
             date_str = item.select_one('.global-teaser__description').get_text(strip=True)
             article_date = date_util(date_str)
             if not article_date:
                 error_message = Error_Message(error_message, "None Date")

             if article_date == today:
                 title = item.select_one('.global-teaser__title').get_text(strip=True)
                 if not title:
                     error_message = Error_Message(error_message, "None Title")

                 article_link = f"https://gov.georgia.gov{item.a['href']}"
                 if not article_link:
                     error_message = Error_Message(error_message, "None Link")

                 wd.get(article_link)
                 article_html = wd.page_source
                 article_soup = BeautifulSoup(article_html, 'html.parser')

                 paragraphs = article_soup.select("main.content-page__main p")
                 article_body = ' '.join(p.get_text(strip=True) for p in paragraphs)
                 if not article_body:
                     error_message = Error_Message(error_message, "None Contents")

                 if error_message != "":
                     error_list.append({
                         'Error Link': url_42,
                         'Error': error_message
                     })
                 else:
                     articles.append({
                         'Title': title,
                         'Link': article_link,
                         'Content': article_body
                     })

 except Exception as e:
     error_list.append({
         'Error Link': url_42,
         'Error': str(e)
     })




 ########################################### <44> ##############################################

 #url_44 = 'https://www.georgia.org/press-releases'   #실험 후 주석처리
 wd = webdriver.Chrome(service=service, options=chrome_options)
 wd.get(url_44)
 time.sleep(5)
 html = wd.page_source
 soup = BeautifulSoup(html, 'html.parser')
 error_message = str()
 #today = datetime(2023, 10, 31).date()     #For Test

 try:
     news_items = soup.select("[class=info]")
     if not news_items:
         error_list.append({
             'Error Link': url_44,
             'Error': "None News"
         })
     else:
         for item in news_items:
             date_str = item.select_one('.date').get_text(strip=True)
             article_date = date_util(date_str)
             if not article_date:
                 error_message = Error_Message(error_message, "None Date")
             if article_date == today:
                 a_tag = item.find('a')
                 if not a_tag: error_message = Error_Message(error_message, "a_tag 못찾음")
                 all_text = a_tag.get_text(strip=True)
                 not_title = a_tag.find('span', class_='date').get_text(strip=True)
                 title = all_text.replace(not_title, '').strip('" ')
                 if not title:
                     error_message = Error_Message(error_message, "None Title")

                 article_link = f"https://www.georgia.org{item.a['href']}"
                 if not article_link:
                     error_message = Error_Message(error_message, "None Link")

                 wd.get(article_link)
                 article_html = wd.page_source
                 article_soup = BeautifulSoup(article_html, 'html.parser')

                 content_div = article_soup.find('div', class_="field field--name-field-main-content-body field--type-text-with-summary field--label-hidden field__item")
                 if not content_div: error_message = Error_Message(error_message, "content_div 못찾음")
                 article_body = content_div.get_text(separator='\n', strip=True)

                 if not article_body:
                     error_message = Error_Message(error_message, "None Contents")

                 if error_message != "":
                     error_list.append({
                         'Error Link': url_44,
                         'Error': error_message
                     })
                 else:
                     articles.append({
                         'Title': title,
                         'Link': article_link,
                         'Content(Raw)': article_body
                     })

 except Exception as e:
     error_list.append({
         'Error Link': url_44,
         'Error': str(e)
     })





 ########################################### <45> ##############################################

 #url_45 = 'https://www.gachamber.com/all-news/'
 wd = webdriver.Chrome(service=service, options=chrome_options)
 wd.get(url_45)
 time.sleep(5)
 html = wd.page_source
 soup = BeautifulSoup(html, 'html.parser')
 error_message = str()
 # today = datetime(2023, 10, 6).date()     #For Test

 try:
     news_items = soup.select(".fl-post-feed-post")
     if not news_items:
         error_list.append({
             'Error Link': url_45,
             'Error': "None News"
         })
     else:
         for item in news_items:
             meta_tag = item.find('div', class_='fl-post-meta')
             parts = meta_tag.get_text(strip=True).split('·')
             date_str = parts[-1].strip()
             article_date = date_util(date_str)

             if not article_date:
                 error_message = Error_Message(error_message, "None Date")

             if article_date == today:
                 title = item.select_one('a').get_text(strip=True)
                 if not title:
                     error_message = Error_Message(error_message, "None Title")

                 article_link = item.find('h2').find('a')['href']
                 if not article_link:
                     error_message = Error_Message(error_message, "None Link")

                 wd.get(article_link)
                 article_html = wd.page_source
                 article_soup = BeautifulSoup(article_html, 'html.parser')

                 parent_div = article_soup.find('div', class_="fl-module fl-module-fl-post-content fl-node-5cb515ec1b22a")
                 paragraphs = parent_div.select(".fl-module-content.fl-node-content p") #if parent_div else []
                 article_body = ' '.join(p.get_text(strip=True) for p in paragraphs)

                 if not article_body:
                     error_message = Error_Message(error_message, "None Contents")

                 if error_message != "":
                     error_list.append({
                         'Error Link': url_45,
                         'Error': error_message
                     })
                 else:
                     articles.append({
                         'Title': title,
                         'Link': article_link,
                         'Content': article_body
                     })

 except Exception as e:
     error_list.append({
         'Error Link': url_45,
         'Error': str(e)
     })




 ########################################### <68> ##############################################

 #url_68 = 'https://ag.ny.gov/press-releases'
 wd = webdriver.Chrome(service=service, options=chrome_options)
 wd.get(url_68)
 time.sleep(5)
 html = wd.page_source
 soup = BeautifulSoup(html, 'html.parser')
 error_message = str()
 #today = datetime(2023, 11, 8).date()     #For Test

 try:
     news_items = soup.find_all('div', class_='views-row')
     if not news_items:
         error_list.append({
             'Error Link': url_68,
             'Error': "None News"
         })
     else:
         for item in news_items:
             date_str = item.select_one(".field-content").get_text(strip=True)
             article_date = date_util(date_str)

             if not article_date:
                 error_message = Error_Message(error_message, "None Date")

             if article_date == today:
                 title = item.select_one('a').get_text(strip=True)

                 if not title:
                     error_message = Error_Message(error_message, "None Title")

                 article_link = f"https://ag.ny.gov{item.a['href']}"
                 if not article_link:
                     error_message = Error_Message(error_message, "None Link")

                 wd.get(article_link)
                 article_html = wd.page_source
                 article_soup = BeautifulSoup(article_html, 'html.parser')

                 content_div = article_soup.find('div', class_="node-content tw-typography tw-container tw-mt-8")
                 article_body = ' '.join(p.get_text() for p in content_div.find_all('p')) #if content_div else "No content available."

                 if not article_body:
                     error_message = Error_Message(error_message, "None Contents")

                 if error_message != "":
                     error_list.append({
                         'Error Link': url_68,
                         'Error': error_message
                     })
                 else:
                     articles.append({
                         'Title': title,
                         'Link': article_link,
                         'Content(Raw)': article_body
                     })

 except Exception as e:
     error_list.append({
         'Error Link': url_68,
         'Error': str(e)
     })





 ########################################### <83> ##############################################

 #url_83 = "https://ncchamber.com/category/chamber-updates/"
 wd = webdriver.Chrome(service=service, options=chrome_options)
 wd.get(url_83)
 time.sleep(5)
 html = wd.page_source
 soup = BeautifulSoup(html, 'html.parser')
 error_message = str()
 today = datetime(2023, 11, 8).date()     #실험용 날짜

 try:
     news_items = soup.select(".entry-article")
     if not news_items:
         error_list.append({
             'Error Link': url_83,
             'Error': "None News"
         })
     else:
         for item in news_items:
             date_str = item.select_one('time[datetime]').get_text(strip=True)
             article_date = date_util(date_str)
             if not article_date:
                 error_message = Error_Message(error_message, "None Date")

             if article_date == today:
                 title = item.select_one('h2 a').get_text(strip=True)
                 if not title:
                     error_message = Error_Message(error_message, "None Title")

                 article_link = f"{item.a['href']}"
                 if not article_link:  # 이 부분이 수정되었습니다. `article_link`가 아니라 `link`를 확인해야 합니다.
                     error_message = Error_Message(error_message, "None Link")

                 wd.get(article_link)
                 article_html = wd.page_source
                 article_soup = BeautifulSoup(article_html, 'html.parser')

                 content = article_soup.select_one(".container-post")
                 paragraphs = content.find_all("p")
                 article_body = ' '.join(p.get_text(strip=True) for p in paragraphs)
                 if not article_body:
                     error_message = Error_Message(error_message, "None Contents")

                 if error_message != "":  # 이 부분이 수정되었습니다. `str()` 대신 빈 문자열 `""`을 사용합니다.
                     error_list.append({
                         'Error Link': url_83,
                         'Error': error_message
                     })
                 else:
                     articles.append({
                         'Title': title,
                         'Link': article_link,
                         'Content(Raw)': article_body
                     })

 except Exception as e:
     error_list.append({
         'Error Link': url_83,
         'Error': str(e)
     })




 ########################################### <88> ##############################################

 #url_88 = 'https://www.governor.virginia.gov/newsroom/news-releases/' 
 wd = webdriver.Chrome(service=service, options=chrome_options)
 wd.get(url_88)
 time.sleep(5)
 html = wd.page_source
 soup = BeautifulSoup(html, 'html.parser')
 error_message = str()
 #today = datetime(2023, 11, 10).date()     #For Test


 try:
     news_items = soup.select(".col-md-12")
     if not news_items:
         error_list.append({
             'Error Link': url_88,
             'Error': "None News"
         })
     else:
         for item in news_items:
             date_str = item.select_one(".date").text.strip().strip('"')
             article_date = date_util(date_str)
             if not article_date:
                 error_message = Error_Message(error_message, "None Date")

             if article_date == today:
                 title = item.select_one(".va-icon--arrow-ext").text
                 if not title:
                     error_message = Error_Message(error_message, "None Title")
                 article_link = f"https://www.governor.virginia.gov{item.a['href']}"
                 if not article_link:
                     error_message = Error_Message(error_message, "None Link")

                 wd.get(article_link)
                 article_html = wd.page_source
                 article_soup = BeautifulSoup(article_html, 'html.parser')

                 article_body = None
                 tables = article_soup.select('table.x_layout.x_layout--1-column')
                 if tables:
                     article_body = ' '.join([table.get_text(strip=True) for table in tables])

                 if not article_body:
                     tables = article_soup.select('table.layout.layout--1-column')
                     if tables:
                         article_body = ' '.join([table.get_text(strip=True) for table in tables])

                 if not article_body:
                     divs = article_soup.select('div.col-lg-12.col-md-12.col-sm-12')
                     if divs:
                         paragraphs = [p.get_text(strip=True) for div in divs for p in div.select('p')]
                         article_body = ' '.join(paragraphs)

                 if not article_body:
                     error_message = Error_Message(error_message, "None Contents")

                 if error_message != "":
                     error_list.append({
                         'Error Link': url_88,
                         'Error': error_message
                     })
                 else:
                     articles.append({
                         'Title': title,
                         'Link': article_link,
                         'Content(Raw)': article_body
                     })

 except Exception as e:
     error_list.append({
         'Error Link': url_88,
         'Error': str(e)
     })



 ########################################### <91> ##############################################

 #url_91 = 'https://vachamber.com/category/press-releases/'
 wd = webdriver.Chrome(service=service, options=chrome_options)
 wd.get(url_91)
 time.sleep(5)
 html = wd.page_source
 soup = BeautifulSoup(html, 'html.parser')
 error_message = str()
 #today = datetime(2023, 11, 9).date()     #For Test

 try:
     news_items = soup.select(".archiveitem")
     if not news_items:
         error_list.append({
             'Error Link': url_91,
             'Error': "None News"
         })
     else:
         for item in news_items:
             date_str = item.select_one('.item_meta').get_text(strip=True)
             article_date = date_util(date_str)
             if not article_date:
                 error_message = Error_Message(error_message, "None Date")

             if article_date == today:
                 title = item.select_one('h4 a').get_text(strip=True)
                 if not title:
                     error_message = Error_Message(error_message, "None Title")

                 article_link = f"{item.a['href']}"
                 if not article_link:
                     error_message = Error_Message(error_message, "None Link")

                 wd.get(article_link)
                 article_html = wd.page_source
                 article_soup = BeautifulSoup(article_html, 'html.parser')

                 content = article_soup.select_one(".content_wrapper_right")
                 if content:
                     news_contact = content.select_one(".newscontact")
                     if news_contact:
                         news_contact.decompose()
                     paragraphs = content.find_all("p")
                     for i, p in enumerate(paragraphs):
                         if "additional highlights" in p.get_text().lower():
                             paragraphs = paragraphs[:i]
                             break
                     article_body = ' '.join(p.get_text(strip=True) for p in paragraphs)

                 if not article_body:
                     error_message = Error_Message(error_message, "None Contents")

                 if error_message != "":
                     error_list.append({
                         'Error Link': url_91,
                         'Error': error_message
                     })
                 else:
                     articles.append({
                         'Title': title,
                         'Link': article_link,
                         'Content(Raw)': article_body
                     })

 except Exception as e:
     error_list.append({
         'Error Link': url_91,
         'Error': str(e)
     })





 ########################################### <93> ##############################################

 #url_93 = 'https://news.maryland.gov/mde/category/press-release/'
 wd = webdriver.Chrome(service=service, options=chrome_options)
 wd.get(url_93)
 time.sleep(5)
 html = wd.page_source
 soup = BeautifulSoup(html, 'html.parser')
 error_message = str()
 #today = datetime(2023, 11, 2).date()     #For Test

 try:
     news_items = soup.select(".type-post")
     if not news_items:
         error_list.append({
             'Error Link': url_93,
             'Error': "None News"
         })
     else:
         for item in news_items:
             li_tag = item.select_one('li')
             title_str = li_tag.select_one('a').text.strip()
             date_str = li_tag.text.replace(title_str, '').strip()
             article_date = date_util(date_str)

             if not article_date:
                 error_message = Error_Message(error_message, "None Date")

             if article_date == today:
                 title = item.select_one('a').get_text(strip=True)

                 if not title:
                     error_message = Error_Message(error_message, "None Title")

                 article_link = f"{item.a['href']}"

                 if not article_link:
                     error_message = Error_Message(error_message, "None Link")

                 wd.get(article_link)
                 article_html = wd.page_source
                 article_soup = BeautifulSoup(article_html, 'html.parser')

                 paragraphs = article_soup.select('.type-post p, .type-post ul')
                 article_body = ' '.join(p.get_text(strip=True) for p in paragraphs)

                 if not article_body:
                     error_message = Error_Message(error_message, "None Contents")

                 if error_message != "":
                     error_list.append({
                         'Error Link': url_93,
                         'Error': error_message
                     })
                 else:
                     articles.append({
                         'Title': title,
                         'Link': article_link,
                         'Content(Raw)': article_body
                     })

 except Exception as e:
     error_list.append({
         'Error Link': url_93,
         'Error': str(e)
